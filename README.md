# ODSC_Op_GenAI

### This repository is for the Talk at ODSC APAC 2024 on "Operationalizing GenAI: Effective LLM Compression and Optimization Methods" on 13th August 2024


<p>In today's rapidly evolving AI landscape, deploying large language models (LLMs) efficiently is a significant challenge, especially when balancing performance with cost. ""Operationalizing GenAI: Effective LLM Compression and Optimization Methods"" delves into cutting-edge techniques to streamline the deployment of generative AI applications. This session is designed for AI practitioners, data scientists, and machine learning engineers who are keen on enhancing their expertise in model optimization and seeking practical solutions for real-world applications.

Attendees will explore a range of LLM compression techniques, including quantization, pruning, and knowledge distillation, and understand how these methods can significantly reduce model size and inference costs without sacrificing performance. The session will also cover optimization strategies that improve model efficiency and speed, making it easier to deploy LLMs across various platforms and environments.

By the end of this session, participants will gain:

1. A comprehensive understanding of different LLM compression techniques and their practical applications.
2, Insights into optimization methods that enhance model deployability and reduce operational costs.
3. Practical knowledge on integrating these methods into existing GenAI workflows to achieve scalable and efficient AI solutions.

This talk is particularly relevant for industries leveraging AI for large-scale applications, such as finance, healthcare, and technology, where optimizing resources and minimizing costs are critical. Attendees will leave equipped with actionable strategies to apply these advanced techniques to their projects, driving significant improvements in both performance and cost-efficiency. Join me to transform the way you deploy and manage generative AI applications, making them more accessible and sustainable for your organization.

Background Knowledge:
Some exposure to LLMs and deploying them will help
</p>

<img>

![APAC2024_Nijesh Kanjinghat](https://github.com/user-attachments/assets/638fb4e6-cabc-4b49-939e-50cc191743d7)


</img>
